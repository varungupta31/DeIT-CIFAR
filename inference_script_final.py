# -*- coding: utf-8 -*-
"""inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aXzb1VEC_rxu-8agYHh0Exl0_VKUcJOj
"""

import torch
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torchvision.models as models
import torch.nn as nn
# import timm
import torch.nn.functional as F
import os
import torchvision.models as models
# from teacher_model_18 import teacher_model
from sklearn.metrics import *
from sklearn.metrics import top_k_accuracy_score
import numpy as np
from vit import VisionTransformer
from deit import DataEfficientImageTransformer as DEIT
torch.manual_seed(123)
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
import yaml
import argparse



parser = argparse.ArgumentParser()
parser.add_argument("--config", help = "path of the config file for inference ", required = True)
args = parser.parse_args()

#Reading the configuration file
with open(args.config, 'r') as f:
    try:
        config = yaml.safe_load(f)
    except yaml.YAMLError as exc:
        print(exc)


def getInferenceHardToken(model, loader,  device):

    model.eval()
    cls_token_acc1_log = 0
    distill_token_acc1_log = 0
    cls_distill_token_acc1_log = 0
    cls_token_precision = 0
    cls_token_recall = 0
    cls_token_f1_score = 0
    distill_token_precision = 0
    distill_token_recall = 0
    distill_token_f1_score = 0
    cls_distill_token_precision = 0
    cls_distill_token_recall = 0
    cls_distill_token_f1_score = 0
    cls_token_acc3_log = 0
    distill_token_acc3_log = 0
    cls_distill_token_acc3_log = 0
    cls_token_acc5_log = 0
    distill_token_acc5_log = 0
    cls_distill_token_acc5_log = 0
    all_gt = []
    all_ground = [] 
    with torch.no_grad():
        for batch_index , (data, gt) in enumerate(loader):
            data = data.to(device)
            gt = gt.to(device)
            scores_cls_token, scores_distill_token = model(data)
            scores_cls_distill_token = scores_cls_token + scores_distill_token

            scores_cls_token =F.softmax(scores_cls_token, dim = 1)
            scores_distill_token = F.softmax(scores_distill_token, dim = 1)
            scores_cls_distill_token = F.softmax(scores_cls_distill_token, dim = 1)

            scores_cls_token = scores_cls_token.cpu().detach().numpy()
            scores_distill_token = scores_distill_token.cpu().detach().numpy()
            scores_cls_distill_token = scores_cls_distill_token.cpu().detach().numpy()

            gt = gt.cpu().detach().numpy()
            if config["dataset"] == "cifar":
                labels = np.arange(0,100)
            elif config["dataset"] == "imagenet32":
                labels = np.arange(0,1000)

            # if acc_mode == "top1":
            cls_token_acc1 = top_k_accuracy_score(gt,scores_cls_token, k=1, labels = labels)*100
            distill_token_acc1 = top_k_accuracy_score(gt, scores_distill_token, k = 1, labels = labels)*100
            cls_distill_token_acc1 = top_k_accuracy_score(gt, scores_cls_distill_token,k = 1, labels = labels)*100

            cls_token_precision += precision_score(gt, np.argmax(scores_cls_token, axis=1), average='macro',zero_division=0)
            cls_token_recall += recall_score(gt, np.argmax(scores_cls_token, axis=1), average='macro')
            cls_token_f1_score += f1_score(gt, np.argmax(scores_cls_token, axis=1), average='macro')

            distill_token_precision += precision_score(gt, np.argmax(scores_distill_token, axis=1), average='macro',zero_division=0)
            distill_token_recall += recall_score(gt, np.argmax(scores_distill_token, axis=1), average='macro')
            distill_token_f1_score += f1_score(gt, np.argmax(scores_distill_token, axis=1), average='macro')

            cls_distill_token_precision += precision_score(gt, np.argmax(scores_cls_distill_token, axis=1), average='macro',zero_division=0)
            cls_distill_token_recall += recall_score(gt, np.argmax(scores_cls_distill_token, axis=1), average='macro')
            cls_distill_token_f1_score += f1_score(gt, np.argmax(scores_cls_distill_token, axis=1), average='macro')


            cls_token_acc3 = top_k_accuracy_score(gt,scores_cls_token, k=3, labels = labels)*100
            distill_token_acc3 = top_k_accuracy_score(gt, scores_distill_token, k = 3, labels = labels)*100
            cls_distill_token_acc3 = top_k_accuracy_score(gt, scores_cls_distill_token,k = 3, labels = labels)*100


            cls_token_acc5 = top_k_accuracy_score(gt,scores_cls_token, k=5, labels = labels)*100
            distill_token_acc5 = top_k_accuracy_score(gt, scores_distill_token, k = 5, labels = labels)*100
            cls_distill_token_acc5 = top_k_accuracy_score(gt, scores_cls_distill_token,k = 5, labels = labels)*100

            # if isPrintBatchAcc == True:
            #     if batch_index % per_batch == 0:
            #       print(f"batch_index: {batch_index}\t acc_cls_token :{cls_token_acc}\t acc_distill_token :{distill_token_acc}\t acc_cls_distill_token :{cls_distill_token_acc}\t"
            #       f"prec_cls_token: {cls_token_precision}\t recall_cls_token: {cls_token_recall}\t f1_cls_score:{cls_token_f1_score}"
            #       f"prec_distill_token:{distill_token_precision}\t recall_distill_token: {distill_token_recall}\t f1_distill_token:{distill_token_f1_score}"
            #       f"prec_class_dist_token:{cls_distill_token_precision}\t recall_class_distill_token:{cls_distill_token_recall}\tf1_class_dist_token:{cls_distill_token_f1_score}")
                  
            cls_token_acc1_log += cls_token_acc1
            distill_token_acc1_log += distill_token_acc1
            cls_distill_token_acc1_log += cls_distill_token_acc1
            cls_token_precision += cls_token_precision
            cls_token_recall += cls_token_recall
            cls_token_f1_score += cls_token_f1_score
            distill_token_precision += distill_token_precision
            distill_token_recall += distill_token_recall
            distill_token_f1_score += distill_token_f1_score
            cls_distill_token_precision += cls_distill_token_precision
            cls_distill_token_recall += cls_distill_token_recall
            cls_distill_token_f1_score += cls_distill_token_f1_score
            cls_token_acc3_log += cls_token_acc3
            distill_token_acc3_log += distill_token_acc3
            cls_distill_token_acc3_log += cls_distill_token_acc3
            cls_token_acc5_log += cls_token_acc5
            distill_token_acc5_log += distill_token_acc5
            cls_distill_token_acc5_log += cls_distill_token_acc5

        return (cls_token_acc1_log/(batch_index+1),
                    distill_token_acc1_log/(batch_index+1),
                    cls_distill_token_acc1_log/(batch_index+1),
                    cls_token_precision/(batch_index+1),
                    cls_token_recall/(batch_index+1),
                    cls_token_f1_score/(batch_index+1),
                    distill_token_precision/(batch_index+1),
                    distill_token_recall/(batch_index+1),
                    distill_token_f1_score /(batch_index+1),
                    cls_distill_token_precision/(batch_index+1),
                    cls_distill_token_recall/(batch_index+1),
                    cls_distill_token_f1_score/(batch_index+1),
                    cls_token_acc3_log/(batch_index+1),
                    distill_token_acc3_log/(batch_index+1),
                    cls_distill_token_acc3_log/(batch_index+1),
                    cls_token_acc5_log/(batch_index+1),
                    distill_token_acc5_log/(batch_index+1),
                    cls_distill_token_acc5_log/(batch_index+1)
                    )

def getInference(model, loader,  device):
  model.eval()
  out_acc1 = 0
  out_acc3 = 0
  out_acc5 = 0
  precision = 0
  recall = 0
  f1Score = 0 
  




  for batch_index ,(data, gt) in enumerate(loader):
   data = data.to(device)
   gt = gt.to(device)
   scores = model(data)
   scores = F.softmax(scores, dim = 1)
   scores = scores.cpu().detach().numpy()
   gt = gt.cpu().detach().numpy()
   if config["dataset"] == "cifar":
    labels = np.arange(0,100)
   else:
    labels = np.arange(0,1000)

   acc1 = top_k_accuracy_score(gt,scores, k=1, labels = labels)*100
   acc3 = top_k_accuracy_score(gt,scores, k=3, labels = labels)*100
   acc5 = top_k_accuracy_score(gt,scores, k=5, labels = labels)*100

   precision += precision_score(gt, np.argmax(scores, axis=1), average='macro',zero_division=0)
   recall += recall_score(gt, np.argmax(scores, axis=1), average='macro')
   f1Score += f1_score(gt, np.argmax(scores, axis=1), average='macro')

   precision += precision
   recall += recall
   f1Score += f1Score


   out_acc1 += acc1
   out_acc3 += acc3
   out_acc5 += acc5
  
  return (out_acc1/(batch_index+1),
                out_acc3/(batch_index+1),
                out_acc5/(batch_index+1),
                precision/(batch_index+1),
                recall/(batch_index+1),
                f1Score/(batch_index+1))

def doInference(loader, mode = "distilled_token", model= None,  device="cuda"):
	if mode == "distilled_token":

		final_model = model
		
		loader = loader
		device = device
		acc_mode = acc_mode

		
		if final_model == None:
			print("No model is provided!!")
		else:
			clsTokenAcc1, distillTokenAcc1, clsDistillTokenAcc1, cls_token_precision,cls_token_recall,cls_token_f1_score, distill_token_precision , distill_token_recall,distill_token_f1_score,cls_distill_token_precision,cls_distill_token_recall,cls_distill_token_f1_score,clsTokenAcc3, distillTokenAcc3, clsDistillTokenAcc3,clsTokenAcc5, distillTokenAcc5, clsDistillTokenAcc5= getInferenceHardToken(final_model,loader, device)
			print("Final Top-1 Accuracy")
	 
			print(f"cls_token_acc:{clsTokenAcc1}\n distill_token_acc:{distillTokenAcc1}\n cls_distill_token_acc:{clsDistillTokenAcc1}\n"
      f"prec_cls_token: {cls_token_precision}\n recall_cls_token: {cls_token_recall}\n f1_cls_score:{cls_token_f1_score}\n"
      f"prec_distill_token:{distill_token_precision}\n recall_distill_token: {distill_token_recall}\n f1_distill_token:{distill_token_f1_score}\n"
			 f"prec_class_dist_token:{cls_distill_token_precision}\n recall_class_distill_token:{cls_distill_token_recall}\nf1_class_dist_token:{cls_distill_token_f1_score}")
	 

			print("************************************")

			print("Final Top-3 Accuracy")
			print(f"cls_token_acc:{clsTokenAcc3}\n distill_token_acc:{distillTokenAcc3}\n cls_distill_token_acc:{clsDistillTokenAcc3}\n")
			print("************************************")

			print("Final Top-5 Accuracy")
			print(f"cls_token_acc:{clsTokenAcc5}\n distill_token_acc:{distillTokenAcc5}\n cls_distill_token_acc:{clsDistillTokenAcc5}\n")
	 

		

	else:
		final_model = model
		# print("entering.....")
		loader = loader
		device = device
		
		if final_model == None:
			print("No model is provided!!")
		else:
            
			acc_1,acc_3,acc_5,precision,recall,f1Score = getInference(final_model,loader,  device)
			print("Final Accuracy.....")
			print(f"acc@1:{acc_1}\t acc@3 :{acc_3}\t acc@5 :{acc_5} precision:{precision}\t recall:{recall}\t f1_score:{f1Score}")
            # print(f"precision:{precision}\t recall:{recall}\t f1_score:{f1_score}")

#apply transformation
if config["dataset"] == "cifar":
    transforms = transforms.Compose([transforms.ToTensor(), 
							transforms.Normalize((0.2675, 0.2565, 0.2761),(0.5071, 0.4867, 0.4408))])
#load the test set
    testset = datasets.CIFAR100(root = config["paths"]["dataset_download_path"], train = False, transform = transforms, download = False)
    batch_size = config["batch_size"]
    test_dataloaders = DataLoader(testset, batch_size = batch_size, shuffle = "False", num_workers = config["num_workers"])

elif config["dataset"] == "imagenet32":
    transforms = transforms.Compose([transforms.ToTensor(),
                            transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))])
    testset = ImageNet32(root = config['paths']['dataset_download_path'], train = True, transform =transforms)
    batch_size = config["batch_size"]
    test_dataloaders = DataLoader(testset, batch_size = batch_size, shuffle = "False", num_workers = config["num_workers"])

#check the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#Initializing the model
# model = models.regnet_y_16gf()
# num_ftrs = model.fc.in_features
# model.fc = nn.Linear(num_ftrs, 100)

if config["model_type"] == "Transformer":

        if config["transformer_config"]["use_default_vit_B"]:

            custom_config = {
                "img_size": 32,
                "in_chans": 3,
                "patch_size": 16,
                "embed_dim": 768,
                "depth": 12,
                "n_heads": 12,
                "qkv_bias": True,
                "mlp_ratio": 4,
            }

        elif config["transformer_config"]["use_default_vit_S"]:

            custom_config = {
                "img_size": 32,
                "in_chans": 3,
                "patch_size": 16,
                "embed_dim": 384,
                "depth": 12,
                "n_heads": 6,
                "qkv_bias": True,
                "mlp_ratio": 4,
            }

        elif config["transformer_config"]["use_default_vit_Ti"]:

            custom_config = {
                "img_size": 32,
                "in_chans": 3,
                "patch_size": 16,
                "embed_dim": 192,
                "depth": 12,
                "n_heads": 3,
                "qkv_bias": True,
                "mlp_ratio": 4,
            }


        # model = VisionTransformer(**custom_config)
        # model = nn.Sequential(model, nn.Linear(1000,100, bias=True))
        if config["mode"] == "distilled_token":
            model = DEIT(**custom_config)



        #calculating the total number of parameters
            total_num_param = sum(p.numel() for p in model.parameters())
            print(f"Total number of parameters for:{total_num_param}")

        #loading the  model
            saved_model_path = config["paths"]["model_path"]
            model.load_state_dict(torch.load(saved_model_path))
            model.to(device)


            doInference(mode = config["mode"], model= model,loader= test_dataloaders)

        elif config["mode"] == "Normal":
            if config["dataset"] == "cifar":
                model = VisionTransformer(**custom_config)
                model = nn.Sequential(model, nn.Linear(1000,100, bias=True))
            elif config["dataset"] == "imagenet32":
                model = VisionTransformer(**custom_config)

            total_num_param = sum(p.numel() for p in model.parameters())
            print(f"Total number of parameters for:{total_num_param}")
            saved_model_path = config["paths"]["model_path"]
            model.load_state_dict(torch.load(saved_model_path))
            model.to(device)


            doInference(mode = config["mode"], model= model,loader= test_dataloaders)

else:
    model = models.regnet_y_16gf()
    total_num_param = sum(p.numel() for p in model.parameters())
    print(f"Total number of parameters for:{total_num_param}")
    saved_model_path = config["paths"]["model_path"]
    model.load_state_dict(torch.load(saved_model_path))
    model.to(device)
    doInference(mode = config["mode"], model= model,loader= test_dataloaders)












